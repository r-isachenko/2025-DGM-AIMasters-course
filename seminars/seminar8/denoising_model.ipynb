{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('seminars/seminar8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dgm_utils import train_model, load_dataset, visualize_images, BaseModel, show_samples\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_dataset('mnist', binarize=False)\n",
    "visualize_images(trainset, title='MNIST samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NCSN, sampling looks like iteratively denoising a noisy image to recover data. But instead of learning the gradient of the log-density â€” what if we directly trained the model to predict the clean image from a noised version?\n",
    "Would that work? What would we need for that?\n",
    "<center><img src=\"pics/ald.gif\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest idea is this: given noise $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$, the model should predict its best estimate of the clean image $\\mathbf{x}$ that could have generated it. Once we have this guess, we slightly reduce the noise level, re-corrupt the current estimate to match the new noise level, and feed it back into the model. Repeating this process multiple times allows the model to progressively denoise the sample, moving step-by-step from pure noise toward a coherent image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider the model. To make these predictions, we need a network that takes a noisy image as input and outputs a denoised version. A commonly used architecture for this task is the [U-Net](https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "<center><img src=\"pics/unet.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.down_layers = nn.ModuleList([ \n",
    "            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "        ])\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
    "            nn.Conv2d(32, out_channels, kernel_size=5, padding=2), \n",
    "        ])\n",
    "        self.act = nn.SiLU() # The activation function\n",
    "        self.downscale = nn.MaxPool2d(2)\n",
    "        self.upscale = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = []\n",
    "        for i, l in enumerate(self.down_layers):\n",
    "            x = self.act(l(x)) # Through the layer and the activation function\n",
    "            if i < 2: # For all but the third (final) down layer:\n",
    "              h.append(x) # Storing output for skip connection\n",
    "              x = self.downscale(x) # Downscale ready for the next layer\n",
    "              \n",
    "        for i, l in enumerate(self.up_layers):\n",
    "            if i > 0: # For all except the first up layer\n",
    "              x = self.upscale(x) # Upscale\n",
    "              x += h.pop() # Fetching stored output (skip connection)\n",
    "            x = self.act(l(x)) # Through the layer and the activation function\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the amount of corruption, we need a smooth way to interpolate between the data and pure noise.\n",
    "\n",
    "When the corruption level is 0, we recover the original input $\\mathbf{x}$ without any change. As the corruption level approaches 1, the sample becomes indistinguishable from pure noise, losing all trace of $\\mathbf{x}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, what should the model do, exactly? Given a corrupted input $x$ the model should output its best guess for what the original $x$ looks like. We will compare this to the actual value via the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingNet(BaseModel):\n",
    "    def __init__(self, input_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.model = BasicUNet(\n",
    "            in_channels=input_shape[0], \n",
    "            out_channels=input_shape[0]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def perturbe(x: torch.Tensor, amount: float, device: str = \"cpu\"):\n",
    "        noise = torch.rand_like(x).to(device)\n",
    "        amount = amount.reshape(-1, 1, 1, 1) # Add new deimensions\n",
    "        return (1 - amount) * x + noise * amount\n",
    "\n",
    "    def forward(self, x):\n",
    "        amount = torch.rand(x.shape[0], device=self.device)\n",
    "        x_noisy = DenoisingNet.perturbe(x, amount, self.device)\n",
    "        return F.mse_loss(x, self.model(x_noisy))\n",
    "    \n",
    "    def loss(self, x: torch.Tensor):\n",
    "        return {\"total_loss\": self(x).mean(dim=0).sum()}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int, num_steps: int = 10, return_trajs: bool = False):\n",
    "        noise = torch.randn(n, *self.input_shape).to(self.device)\n",
    "        x_clean = self.model(noise)\n",
    "        if return_trajs:\n",
    "            traj = [noise.clip(0, 1).cpu()]\n",
    "            guesses = [x_clean.clip(0, 1).cpu()]\n",
    "        \n",
    "        for i in reversed(range(num_steps - 1)):\n",
    "            amount = torch.tensor([i / (num_steps - 1)] * n).to(self.device)\n",
    "            x_noisy = DenoisingNet.perturbe(x_clean, amount, self.device)\n",
    "            x_clean = self.model(x_noisy)\n",
    "            if return_trajs:\n",
    "                traj.append(x_noisy.clip(0, 1).cpu())\n",
    "                guesses.append(x_clean.clip(0, 1).cpu())\n",
    "\n",
    "        if return_trajs:\n",
    "            return guesses, traj\n",
    "        return x_clean.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_perturbe = torch.from_numpy(testset[:10])\n",
    "show_samples(samples_to_perturbe, title='Input samples', nrow=10, figsize=(20, 2))\n",
    "amount = torch.linspace(0, 1, 10)\n",
    "\n",
    "perturbed_samples = DenoisingNet.perturbe(samples_to_perturbe, amount)\n",
    "show_samples(perturbed_samples, title='Perturbed samples', nrow=10, figsize=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noise amount approaches one, our data begins to look like pure random noise. But for most noise amounts, you can guess the digit fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenoisingNet().to(device)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    visualize_samples=True,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses, traj = model.sample(10, return_trajs=True)\n",
    "guesses = torch.stack(guesses, dim=0).reshape(-1, 1, 28, 28).cpu()\n",
    "traj = torch.stack(traj, dim=0).reshape(-1, 1, 28, 28).cpu()\n",
    "\n",
    "show_samples(guesses, title='Input samples', nrow=10, figsize=(12, 12))\n",
    "show_samples(traj, title='Perturbed samples', nrow=10, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_perturbe = torch.from_numpy(testset[:10])\n",
    "show_samples(samples_to_perturbe, title='Input samples', nrow=10, figsize=(20, 2))\n",
    "amount = torch.linspace(0, 1, 10)\n",
    "\n",
    "perturbed_samples = DenoisingNet.perturbe(samples_to_perturbe, amount)\n",
    "show_samples(perturbed_samples, title='Perturbed samples', nrow=10, figsize=(20, 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    denoised_samples = model.model(perturbed_samples.to(device)).cpu().clip(0, 1)\n",
    "show_samples(denoised_samples, title='Denoised samples', nrow=10, figsize=(20, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
