{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('seminars/seminar8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as TD\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from dgm_utils import train_model\n",
    "from seminar8_utils import ScoreMatcher, NoiseConditionedScoreNetwork\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "We will look at:\n",
    "1. **Slow mixing** of Langevin dynamics;\n",
    "2. **Inaccurate score estimation** with score matching;\n",
    "3. How NCSN solves the both problems;\n",
    "4. How to choose the **hyperparameters** for NCSN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define our toy dataset wich is gaussian mixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, probs: List, mus: List[List], sigmas: List[List], device: str=\"cpu\") -> None:\n",
    "        assert sum(probs) == 1.\n",
    "        assert len(probs) == len(mus) == len(sigmas)\n",
    "        self.probs = torch.tensor(probs, device=device)\n",
    "        self.mus = torch.tensor(mus, device=device)\n",
    "        self.sigmas = torch.stack([\n",
    "            torch.diag(torch.tensor(sigma, device=device))\n",
    "            for sigma in sigmas\n",
    "        ])\n",
    "\n",
    "        self.gmm = TD.MixtureSameFamily(\n",
    "            TD.Categorical(self.probs),\n",
    "            TD.MultivariateNormal(self.mus, covariance_matrix=self.sigmas)\n",
    "        )\n",
    "    \n",
    "    def sample(self, shape: Tuple) -> Tensor:\n",
    "        return self.gmm.sample(shape)\n",
    "\n",
    "    def log_prob(self, samples: Tensor) -> Tensor:\n",
    "        return self.gmm.log_prob(samples)\n",
    "\n",
    "    def prob(self, samples: Tensor) -> Tensor:\n",
    "        return self.log_prob(samples).exp()\n",
    "    \n",
    "    def score(self, samples: Tensor) -> Tensor:\n",
    "        with torch.enable_grad():\n",
    "            samples = samples.detach()\n",
    "            samples.requires_grad_(True)\n",
    "            log_prob = self.log_prob(samples).sum()\n",
    "            return torch.autograd.grad(log_prob, samples)[0]\n",
    "  \n",
    "        \n",
    "class InfiniteDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize an iterator over the dataset.\n",
    "        self.dataset_iterator = super().__iter__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch = next(self.dataset_iterator)\n",
    "        except StopIteration:\n",
    "            # Dataset exhausted, use a new fresh iterator.\n",
    "            self.dataset_iterator = super().__iter__()\n",
    "            batch = next(self.dataset_iterator)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.1, 0.90]\n",
    "mus = np.asarray([\n",
    "    [-1., -1.],\n",
    "    [1., 1.]\n",
    "]) * 5\n",
    "sigmas = np.asarray([\n",
    "    [1., 1.],\n",
    "    [1., 1.]\n",
    "])\n",
    "gmm = GMM(probs, mus, sigmas, device=device)\n",
    "\n",
    "samples = gmm.sample((1000,)).cpu()\n",
    "sns.scatterplot(x=samples[:, 0], y=samples[:, 1])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Samples of the GMM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And not the Langevine dynamics it self:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangevinDynamics:\n",
    "    def __init__(self, score_fn: Callable, step_size: Tensor) -> None:\n",
    "        self.score_fn = score_fn\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def step(self, xt: Tensor, **kwargs: Dict) -> Tensor:\n",
    "        noise = torch.randn_like(xt)\n",
    "        return (\n",
    "            xt + \n",
    "            0.5 * self.step_size * self.score_fn(xt, **kwargs) +\n",
    "            self.step_size.sqrt() * noise\n",
    "        )\n",
    "\n",
    "    def __call__(self, xt: Tensor, num_steps: int, **kwargs) -> Tensor:\n",
    "        for _ in range(num_steps):\n",
    "            xt = self.step(xt, **kwargs)\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Slow mixing of Langevin dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two modes of the data distribution are separated by low density reegions, Langevin Dymamics will not be able to correctly recover the relative weights of these two modes in reasonable time, and therefore might not converge to the true distribution.\n",
    "\n",
    "Consider a mixture distribution\n",
    "$p_{\\text{data}}(x) = \\alpha p_1(x) + (1 - \\alpha) p_2(x)$, where $p_1(x)$ and $p_2(x)$ are normalized distributions with disjoint supports, and $\\alpha \\in (0, 1)$. Lets look at the gradients in supports of $p_1(x)$ and $p_2(x)$:\n",
    "\n",
    "1. $p_1(x)$: $\\nabla_x \\log p_{\\text{data}}(x) = \\nabla_x (\\log \\alpha + \\log p_1(x)) = \\nabla_x \\log p_1(x)$,\n",
    "2. $p_2(x)$: $\\nabla_x \\log p_{\\text{data}}(x) = \\nabla_x (\\log (1 - \\alpha) + \\log p_2(x)) = \\nabla_x \\log p_2(x)$.\n",
    "\n",
    "In either case, the score $\\nabla_x \\log p_{\\text{data}}(x)$ **does not depend** on $\\alpha$.\n",
    "\n",
    "Since Langevin Dynamics use $\\nabla_x \\log p_{\\text{data}}(x)$ to sample from $p_{\\text{data}}(x)$, the samples obtained will not depend on $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for x and y values for grid points\n",
    "x = np.linspace(-10, 10, 50)\n",
    "y = np.linspace(-10, 10, 50)\n",
    "\n",
    "# Create a meshgrid for x and y values\n",
    "X, Y = np.meshgrid(x, y)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "positions_tensor = torch.tensor(positions, dtype=torch.float32)\n",
    "\n",
    "# Calculate the score for each grid point\n",
    "score = gmm.score(positions_tensor.to(device)).cpu()\n",
    "\n",
    "# Convert score to numpy arrays for plotting\n",
    "U = score[:, 0].detach().numpy().reshape(X.shape)  # x-component of the score\n",
    "V = score[:, 1].detach().numpy().reshape(Y.shape)  # y-component of the score\n",
    "\n",
    "# Plot: Vector field of the score function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(X, Y, U, V, color=\"blue\", alpha=0.75)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Score Function Vector Field of the GMM\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = LangevinDynamics(step_size=torch.tensor(0.01), score_fn=gmm.score)\n",
    "\n",
    "num_steps = 1000\n",
    "x_start = (torch.rand((1000, 2)) - 0.5) * 16.\n",
    "generations = ld(x_start.to(device), num_steps).cpu()\n",
    "sns.scatterplot(x=generations[:, 0], y=generations[:, 1])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Samples of the GMM using Langevin Dynamics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-estimated score in low data density regions\n",
    "\n",
    "Pay your attention to the fact, that we estimate the score-function only nearby to our dataset, which according to the manifold hypothesis lies on a low dimensional manifold.\n",
    "\n",
    "That means, that our trained score-model is ill-estimated at most of the $\\mathbb{R}^D$. As a result, starting from random point and using Langevin Dynamics we are highly probable to fail to converge to the given distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the Score Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, hidden_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = ScoreModel(2, 128).to(device)\n",
    "estimate_ld = LangevinDynamics(step_size=torch.tensor(0.1), score_fn=score_model)\n",
    "\n",
    "matcher = ScoreMatcher(\n",
    "    score_model=score_model, \n",
    "    input_shape=(2,), \n",
    "    score_fn=gmm.score,\n",
    "    langevin_dynamics=estimate_ld\n",
    ")\n",
    "\n",
    "train_loader = InfiniteDataLoader(samples.to(torch.float32), batch_size=128, shuffle=True)\n",
    "test_loader = InfiniteDataLoader(samples.to(torch.float32), batch_size=128, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(matcher.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    matcher,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    visualize_samples=False,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "generations = matcher.sample(1000, num_steps=num_steps).cpu()\n",
    "\n",
    "sns.scatterplot(x=generations[:, 0], y=generations[:, 1])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Samples of the GMM using Langevin Dynamics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for x and y values for grid points\n",
    "x = np.linspace(-10, 10, 50)\n",
    "y = np.linspace(-10, 10, 50)\n",
    "\n",
    "# Create a meshgrid for x and y values\n",
    "X, Y = np.meshgrid(x, y)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "positions_tensor = torch.tensor(positions, dtype=torch.float32)\n",
    "\n",
    "# Calculate the score for each grid point\n",
    "score = score_model(positions_tensor.to(device)).cpu()\n",
    "\n",
    "# Convert score to numpy arrays for plotting\n",
    "U = score[:, 0].detach().numpy().reshape(X.shape)  # x-component of the score\n",
    "V = score[:, 1].detach().numpy().reshape(Y.shape)  # y-component of the score\n",
    "\n",
    "# Plot: Vector field of the score function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(X, Y, U, V, color=\"blue\", alpha=0.75)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Estimated Score Function Vector Field of the GMM\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution of the problems using NCSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main idea** is perturbing data with Gaussian noise.\n",
    "\n",
    "1. Since the support of Gaussian noise distribution is the whole space, the perturbed data will not be confined to a low dimensional manifold, which obviates difficulties from the manifold hypothesis and makes score estimation well-defined.\n",
    "\n",
    "2. Large Gaussian noise has the effect of filling low density regions in the original unperturbed data distribution; therefore score matching may get more training signal to improve score estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalScoreModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, sigma_emb_dim: int, hidden_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma_emb = nn.Sequential(\n",
    "            nn.Linear(1, sigma_emb_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(sigma_emb_dim, sigma_emb_dim),\n",
    "        )\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim + sigma_emb_dim, hidden_dim),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_dim, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, sigma: Tensor) -> Tensor:\n",
    "        sigma = sigma.float().unsqueeze(1)\n",
    "        sigma_emb = self.sigma_emb(sigma)\n",
    "        x = torch.cat([x, sigma_emb], dim=1)\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_score_model = ConditionalScoreModel(2, 16, 128).to(device)\n",
    "estimate_ld = LangevinDynamics(step_size=torch.tensor(0.1), score_fn=cond_score_model)\n",
    "\n",
    "sigma_1 = 8.\n",
    "sigma_L = 0.1\n",
    "L = 10\n",
    "sigmas = [\n",
    "    sigma_1 * (sigma_L/sigma_1)**((i)/(L-1)) for i in range(L)\n",
    "]\n",
    "matcher = NoiseConditionedScoreNetwork(\n",
    "    score_model=cond_score_model, \n",
    "    input_shape=(2,), \n",
    "    sigmas=sigmas,\n",
    "    langevin_dynamics=estimate_ld\n",
    ")\n",
    "\n",
    "train_loader = InfiniteDataLoader(samples.to(torch.float32), batch_size=128, shuffle=True)\n",
    "test_loader = InfiniteDataLoader(samples.to(torch.float32), batch_size=128, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(matcher.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    matcher,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    visualize_samples=False,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "generations = matcher.sample(1000, num_steps=num_steps).cpu()\n",
    "\n",
    "sns.scatterplot(x=generations[:, 0], y=generations[:, 1])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Samples of the GMM using Langevin Dynamics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for x and y values for grid points\n",
    "x = np.linspace(-10, 10, 50)\n",
    "y = np.linspace(-10, 10, 50)\n",
    "\n",
    "# Create a meshgrid for x and y values\n",
    "X, Y = np.meshgrid(x, y)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "positions_tensor = torch.tensor(positions, dtype=torch.float32)\n",
    "\n",
    "for sigma in sigmas:\n",
    "    # Calculate the score for each grid point\n",
    "    sigma = torch.full(size=(positions_tensor.shape[0],), fill_value=sigma, device=device)\n",
    "    score = cond_score_model(positions_tensor.to(device), sigma).cpu()\n",
    "\n",
    "    # Convert score to numpy arrays for plotting\n",
    "    U = score[:, 0].detach().numpy().reshape(X.shape)  # x-component of the score\n",
    "    V = score[:, 1].detach().numpy().reshape(Y.shape)  # y-component of the score\n",
    "\n",
    "    # Plot: Vector field of the score function\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.quiver(X, Y, U, V, color=\"blue\", alpha=0.75)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(f\"Estimated Score Function Vector Field of the GMM $\\sigma={sigma[0].item():.2f}$\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose hyperparameters?\n",
    "Details in the [paper](https://arxiv.org/abs/2006.09011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 0**:\n",
    "choose $\\sigma_L$ in a way that $q_{\\sigma_1}(x) \\approx \\pi(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 1** (Initial noise scale):\n",
    "choose $\\sigma_1$ to be as large as the maximum Euclidean distance between all pairs of training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2** (Other noise scales):\n",
    "choose $\\{\\sigma_i\\}$ as a geometric prograssion with a common ratio $\\gamma$,\n",
    "such that $\\Phi(\\sqrt{2D}(\\gamma - 1) + 3 \\gamma) - \\Phi(\\sqrt{2D}(\\gamma - 1) - 3 \\gamma) \\approx 0.5$, where $D$ is a data dimensionality (should be large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 3** (selecting $T$ and $\\epsilon$):\n",
    "choose $T$ as large as allowed by a computing budget and then select an\n",
    "$\\epsilon$ that makes equation below maximally close to 1.\n",
    "\n",
    "$$\n",
    "    f(\\epsilon)\n",
    "    =\n",
    "    \\left(\n",
    "        1 - \\frac{\\epsilon}{\\sigma_L^2}\n",
    "    \\right)^{2T}\n",
    "    \\left(\n",
    "        \\gamma^2\n",
    "        -\n",
    "        \\frac{\n",
    "            2 \\epsilon\n",
    "        }{\n",
    "            \\sigma_L^2\n",
    "            -\n",
    "            \\sigma_L^2\n",
    "            (1 - \\frac{\\epsilon}{\\sigma_L^2})^2\n",
    "        }\n",
    "    \\right)\n",
    "    +\n",
    "    \\frac{\n",
    "        2 \\epsilon\n",
    "    }{\n",
    "        \\sigma_L^2\n",
    "        -\n",
    "        \\sigma_L^2\n",
    "        (1 - \\frac{\\epsilon}{\\sigma_L^2})^2\n",
    "    }\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
