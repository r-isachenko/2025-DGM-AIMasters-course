{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12465c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 9</center>\n",
    "\n",
    "<center>24.04.2025</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2d499",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4633cd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pics/diffusion_pgm.png\" width=1000 /></center>\n",
    "\n",
    "Diffusion models are latent variable models of the form\n",
    "$$\n",
    "p_\\theta(x_0) := \\int p_\\theta(x_0 | z) p_\\theta(z) dz,\n",
    "$$\n",
    "where $z = (x_1, \\dots, x_T)$ are latents of the same dimensionality as the data $x_0 \\sim q(x_0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a34e6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 1.** How we can define encoder in such VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f392efa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case, the most convenient way to build an encoder is to fix a Markov chain that gradually adds Gaussian noise to the data, according to  \n",
    "$$\n",
    "    q(z | x) = q(x_1, \\dots, x_T | x_0) = \\prod_{t = 1}^T q(x_t | x_{t - 1}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) := \\mathcal{N}\\left(x_t; \\sqrt{1 - \\beta_t} \\, x_{t-1}, \\beta_t \\, \\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "**Sub-question:**  \n",
    "Why do we need to add a Gaussian forward process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e068c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. $q(x_{\\infty} \\mid x_0) = \\mathcal{N}(0, I)$ — by repeatedly multiplying by a factor $0 < \\sqrt{1 - \\beta} < 1$, we progressively blur the original image.  \n",
    "2. $p_{\\infty}(x) = \\mathcal{N}(0, I)$ — the mean is driven to zero, and we inject a large amount of Gaussian noise — resulting in pure Gaussian noise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3958b0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 2.** Why do we need Markov property?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5674af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Markov property of $q(x_t \\mid x_{t-1})$ is mainly due to the simplicity it provides in constructing the forward process. Now, we have an analytic formula that directly applies noise to the image from $x_0$ to $x_t$:\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) \\, \\mathbf{I} \\right),\n",
    "$$\n",
    "where $\\bar{\\alpha}_t = \\prod_{s=1}^t (1 - \\beta_s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a72e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 3.** How would we train such \"VAE\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3d31d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course it is ELBO:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ -\\log p_\\theta(\\mathbf{x}_0) \\right] \n",
    "\\leq \\mathbb{E}_q \\left[ \n",
    "    -\\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\mid \\mathbf{x}_0)} \n",
    "\\right] \n",
    "= \n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\mathbb{E}_q \\left[\n",
    "D_{\\mathrm{KL}}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) \\,\\|\\, p(\\mathbf{x}_T))\n",
    "+ \\sum_{t > 1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\,\\|\\, p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))\n",
    "- \\log p_\\theta(\\mathbf{x}_0 \\mid \\mathbf{x}_1)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "So, what we actually need to minimize is the KL divergence between the true posterior of the forward process and the learnable reverse process:\n",
    "\n",
    "$$\n",
    "\\text{KL}\\left(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\,\\|\\, p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)\\right),\n",
    "$$\n",
    "\n",
    "where the posterior $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)$ can be derived analytically using Bayes’ rule, since all involved distributions are Gaussian:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0 + \\frac{\\sqrt{1 - \\beta_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t, \\frac{(1 - \\bar{\\alpha}_{t-1}) \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{I}),\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f9602",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 4.** How to parametrize $p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb120b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define the reverse transition as a Gaussian:\n",
    "$$\n",
    "p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t))\n",
    "$$\n",
    "\n",
    "**Mean $\\mu_\\theta$** is predicted by a neural network conditioned on $\\mathbf{x}_t$ and timestep $t$\n",
    "\n",
    "**Variance $\\Sigma_\\theta$** is fixed to a diagonal matrix $\\sigma_t^2 \\mathbf{I}$ with $\\sigma_t$ having two choises:\n",
    "1. **$ \\sigma_t^2 = \\beta_t $**  \n",
    "   - This matches the **forward process noise** at each step.  \n",
    "   - **Optimal when** the data $ \\mathbf{x}_0 $ is assumed to follow a standard Gaussian: $$ \\mathbf{x}_0 \\sim \\mathcal{N}(0, \\mathbf{I}) $$\n",
    "   - Leads to **higher entropy** (more uncertainty) in the reverse step.\n",
    "1. **$ \\sigma_t^2 = \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t $**  \n",
    "   - Derived from the posterior of the forward process.  \n",
    "   - **Optimal when** $ \\mathbf{x}_0 $ is **fixed** (i.e., no uncertainty).  \n",
    "   - Produces **lower entropy**, more confident reverse predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d29128",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 5.** How we can simplify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48ab90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Indeed sinse we know KL between Normal distribtuions and $\\sigma$ is fixed we can just:\n",
    "\n",
    "$$\n",
    "L_{t-1} = \\mathbb{E}_q \\left[\n",
    "\\frac{1}{2\\sigma_t^2} \\left\\| \\tilde{\\mu}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\mu_\\theta(\\mathbf{x}_t, t) \\right\\|^2\n",
    "\\right]\n",
    "= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[\n",
    "\\frac{1}{2\\sigma_t^2} \\left\\|\n",
    "\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right) \n",
    "- \\mu_\\theta(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t)\n",
    "\\right\\|^2\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) = \\sqrt{\\bar{\\alpha}_t} \\, \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e583a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since $ \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) $ is available, we reparameterize the reverse mean as:\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon_\\theta(\\mathbf{x}_t, t) \\right)\n",
    "$$\n",
    "\n",
    "where $ \\epsilon_\\theta(\\mathbf{x}_t, t) $ predicts the noise $ \\epsilon $ added during the forward process.\n",
    "\n",
    "Instead of matching the posterior mean, we directly train $ \\epsilon_\\theta $ to predict noise using the following loss:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[\n",
    "\\frac{\\beta_t^2}{2 \\sigma_t^2 \\alpha_t (1 - \\bar{\\alpha}_t)} \n",
    "\\left\\| \\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t) \\right\\|^2\n",
    "\\right] = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \n",
    "\\left\\| \\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t) \\right\\|^2\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ded9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 6.** Why do we need to make small steps to have good quality of samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922083c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we sample at first:\n",
    "\n",
    "$$\n",
    "x_{t-1} =\n",
    "\\frac{1}{\\sqrt{1 - \\beta_t}} \n",
    "\\left(\n",
    "x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon_\\theta(x_t, t)\n",
    "\\right)\n",
    "+\n",
    "\\sqrt{\\beta_t} z\n",
    "$$\n",
    "\n",
    "\n",
    "Remember initially we have Gaussian assumption:\n",
    "$$\n",
    "p(x_{t-1} \\mid x_t) \\approx \\mathcal{N}(\\mu_\\theta(x_t, t), \\sigma_t^2 \\mathbf{I})\n",
    "$$\n",
    "\n",
    "Lets look at fromula from lecture that follows from Bayes’ rule:\n",
    "$$\n",
    "p(x_{t-1} \\mid x_t) = \\frac{q(x_t \\mid x_{t-1}) \\, q(x_{t-1})}{q(x_t)}\n",
    "$$\n",
    "\n",
    "If the noise added in each step is **small** (small $ \\beta_t $), the distributions are **locally smooth** and the log-density can be approximated by a **second-order Taylor expansion** resulting in a **Gaussian** reverse kernel. See **Feller theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b7c39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Qustion 7.** How to make DDPM faster?\n",
    "<center><img src=\"pics/ddim.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae1ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To increaset the speed lets consider non-Markov forward process (encoder):\n",
    "\n",
    "$$\n",
    "q_\\sigma(x_{1:T} \\mid x_0) := q_\\sigma(x_T \\mid x_0) \\prod_{t=2}^T q_\\sigma(x_{t-1} \\mid x_t, x_0),\n",
    "$$\n",
    "\n",
    "where terminal distribution is $q_\\sigma(x_T \\mid x_0) = \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_T} x_0, (1 - \\bar{\\alpha}_T) \\mathbf{I} \\right),$\n",
    "and for all $ t > 1 $, the transitions are:\n",
    "$$\n",
    "q_\\sigma(x_{t-1} \\mid x_t, x_0) = \\mathcal{N} \\left(\n",
    "\\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{\\sqrt{1 - \\bar{\\alpha}_t}},\n",
    "\\; \\sigma_t^2 \\, \\mathbf{I}\n",
    "\\right)\n",
    "$$\n",
    "**Note:**  \n",
    "The mean function in $ q_\\sigma(x_{t-1} \\mid x_t, x_0) $ is carefully chosen to ensure that the marginal distribution remains consistent with the original forward process:\n",
    "$$\n",
    "q_\\sigma(x_t \\mid x_0) = \\mathcal{N}\\left( \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) \\, \\mathbf{I} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468b1b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To minimize the they similarly derive ELBO:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q_\\sigma} \\left[\n",
    "\\log q_\\sigma(x_T \\mid x_0) + \\sum_{t=2}^{T} \\log q_\\sigma(x_{t-1} \\mid x_t, x_0)\n",
    "- \\sum_{t=1}^{T} \\log p_\\theta^{(t)}(x_{t-1} \\mid x_t)\n",
    "- \\log p_\\theta(x_T)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "In the same manner they derive loss in noise prediction manner. Moreover, authors prooved **this loss is eqivalent to DDPM loss**, which means we can use previously train DDPM to apply following sampling scheme:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \n",
    "\\sqrt{\\bar{\\alpha}_{t-1}} \n",
    "\\left(\n",
    "\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta^{(t)}(x_t)}{\\sqrt{\\bar{\\alpha}_t}}\n",
    "\\right)\n",
    "+\n",
    "\\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta^{(t)}(x_t)\n",
    "+\n",
    "\\sigma_t \\epsilon_t\n",
    "$$\n",
    "\n",
    "**Note:** when the noise schedule is set to $\\sigma_t = \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}} \\cdot \\sqrt{1 - \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}},$\n",
    "for all $t$, the forward process becomes **Markovian**, and the overall generative model reduces to a **DDPM**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
