{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f392a708-cc19-4919-8041-952d6ed3cb3f",
   "metadata": {},
   "source": [
    "# $\\beta$-VAE and the Relationship Between $\\beta$ and $\\sigma$\n",
    "\n",
    "## $\\beta$-VAE\n",
    "\n",
    "Beta-VAE is a variation of the standard Variational Autoencoder (VAE) that introduces a regularization coefficient $\\beta$ to the Kullback-Leibler divergence term in the VAE loss function.\n",
    "\n",
    "In a standard VAE, the loss function takes the form:\n",
    "\n",
    "$$L = Reconstruction Error + DKL(q(z|x) || p(z))$$\n",
    "\n",
    "\n",
    "In $\\beta$-VAE, the loss function is modified as:\n",
    "\n",
    "$$L = Reconstruction Error + \\beta * DKL(q(z|x) || p(z))$$\n",
    "\n",
    "\n",
    "where $\\beta$ is a positive coefficient that controls the strength of regularization.\n",
    "\n",
    "## Key Features of Beta-VAE\n",
    "\n",
    "1. When $\\beta > 1$, the model places greater emphasis on regularization, which promotes learning more disentangled and interpretable latent representations.\n",
    "\n",
    "2. $\\beta$-VAE was designed to address the problem of learning disentangled representations — where individual dimensions of the latent space correspond to separate factors of variation in the data.\n",
    "\n",
    "3. Higher values of β can lead to better disentanglement of factors, but at the cost of reduced reconstruction quality.\n",
    "\n",
    "## Relationship Between Beta and Sigma\n",
    "\n",
    "There is a specific relationship between the $\\beta$ parameter and the variance $\\sigma^{2}$ in the latent space:\n",
    "\n",
    "1. In standard VAE, we typically assume that the prior distribution of the latent space $p(z)$ is a standard normal distribution $N(0, I)$, where I is the identity matrix. When we increase the value of β in Beta-VAE, we effectively increase the weight of the KL-divergence regularization term.\n",
    "\n",
    "2. As $\\beta$ increases, the model tries to make $q(z|x)$ (the encoder) closer to the prior distribution $p(z)$.\n",
    "\n",
    "3. This means that the variance $\\sigma^{2}$ of the distribution $q(z|x)$ becomes closer to one, and the mean value $\\mu$ tends toward zero.\n",
    "\n",
    "4. When $\\beta$ is very high, the encoder will strongly tend to output mean values close to zero and variances close to one for each dimension of the latent space.\n",
    "\n",
    "5. In some implementations of $\\beta$-VAE, instead of directly changing $\\beta$, researchers sometimes adjust the variance $\\sigma^{2}$ of the prior distribution $p(z) = N(0, \\sigma^{2}I)$. Mathematically, changing the variance of the prior distribution has an effect similar to changing β (though not identical).\n",
    "\n",
    "Thus, a high $\\beta$ value can be informally viewed as a way to \"compress\" the variance of the distribution $q(z|x)$, making it more constrained and closer to the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2eda2d-89e3-400b-922a-19d941339df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
