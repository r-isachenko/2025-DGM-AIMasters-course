{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZXJoDiD_x-N"
   },
   "source": [
    "# Homework5: Denoising Diffusion Probabilistic Model (DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxF8ewFXn1HO"
   },
   "source": [
    "## Task 1: Theory (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN6DycPP2Xq2"
   },
   "source": [
    "### Problem 1: Gaussian Diffusion (2pt)\n",
    "\n",
    "In the course we have discussed two types of Gaussian diffusions:\n",
    "- $\\mathbf{x}_t = \\mathbf{x}_0 + \\sigma_t \\cdot \\boldsymbol{\\epsilon}$ - score-based models,\n",
    "- $\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\cdot \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\cdot \\boldsymbol{\\epsilon}$ - diffusion models.\n",
    "\n",
    "**Part 1.** Let consider the single step Markov chain $q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(a \\cdot \\mathbf{x}_{t-1}, b^2 \\cdot \\mathbf{I})$. We can choose $a$ and $b$ such that the distribution of $\\mathbf{x}_{\\infty}$ has the identity covariance matrix. Such a process is called *Variance Preserving*. Prove that for this property to hold, it is necessary that:\n",
    "$$\n",
    "a = \\sqrt{\\alpha}; \\quad b = \\sqrt{1 − \\alpha}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Part 2.** One may ask: \"Why we do not consider the more general diffusion models?\". This was exactly the idea of the paper [Variational Diffusion Models](https://arxiv.org/abs/2107.00630).\n",
    "\n",
    "Let consider the diffusion of the form\n",
    "$$\n",
    "    \\mathbf{x}_t = \\alpha_t \\cdot \\mathbf{x}_0 + \\sigma_t \\cdot \\boldsymbol{\\epsilon}, \\quad \\mathbf{x}_t \\sim q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\alpha_t \\cdot \\mathbf{x}_0, \\sigma_t^2 \\cdot \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "Find the distribution $q(\\mathbf{x}_t | \\mathbf{x}_s)$ for $s < t$ (you have to derive the formulas for mean $\\alpha_{t|s}$ and variance $\\sigma_{t|s}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbxPyzwcuinj"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Strided sampling (2pt)\n",
    "\n",
    "Sampling from DDPM is very slow.  There are several techniques to alleviate this drawback. \n",
    "In this task we are going to investigate one of them.\n",
    "\n",
    "Assume we have already trained a model $p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta})$ to \"reverse\" a Markov chain of length $T$. Let's try to build inference process using subsequence of timesteps $\\{S_0 = 0, S_1, \\ldots, S_{T'-1}, S_{T'} = T\\}$, where $T' < T$. Using this subsequence we have to do $T'$ inference steps instead of $T$. It could dramatically reduce inference time.\n",
    "\n",
    "You goal is to find the expression for the iterative update in this case (how to get $\\mathbf{x}_{S_{t-1}}$ from $\\mathbf{x}_{S_t}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Tweedie’s formula (1 pt)\n",
    "\n",
    "Consider a Wiener process $dX=\\sigma\\,dW$ initiated from a distribution $p_0(x)$. Now recall that if we run the given Wiener process for a duration $\\delta>0$ we have  \n",
    "\n",
    "$$\n",
    "    X_\\delta = X_0 + \\mathcal{N}(0,\\sigma^{2}\\delta),\n",
    "$$\n",
    "\n",
    "where $X_0 \\sim p_0(x)$.\n",
    "\n",
    "We know that the reverse dynamics are given by $d\\bar{X}=\\sigma^{2}\\nabla\\log p_t(\\bar{X})\\,dt + \\sigma\\,d\\bar{W}$. Thus, fixing some $y = X_\\delta$ for small $\\delta$, the following approximation will hold true:\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}\\!\\left[X_0 \\mid X_\\delta = y\\right] \\approx y + \\nabla\\log p_\\delta(y)\\,\\sigma^{2}\\delta. \\tag{$*$}\n",
    "$$\n",
    "\n",
    "Notably, for a Wiener process this relationship holds **exactly**, even for arbitrarily large time increments $\\delta>0$. This is guaranteed by Tweedie’s formula.\n",
    "\n",
    "From the construction of $X_\\delta$ we can see that it has the following density:\n",
    "\n",
    "$$\n",
    "    p_{X_\\delta}(y)=\\bigl(p_0 * \\rho_\\delta\\bigr)(y)=\\int p_0(x)\\,\\rho_\\delta(y-x)\\,dx.\n",
    "$$\n",
    "\n",
    "Given these facts, your goal is to prove Tweedie’s formula, i.e., $(*)$ with equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jppIpVyZ2Xq3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "629ad74a-6fad-434d-b7e5-e19ae589f687"
   },
   "outputs": [],
   "source": [
    "COMMIT_HASH = \"1450816a075b004a60dc67e108a6a842ad71a4b0\"\n",
    "!if [ -d dgm_utils ]; then rm -Rf dgm_utils; fi\n",
    "!git clone https://github.com/r-isachenko/dgm_utils.git\n",
    "%cd dgm_utils\n",
    "!git checkout {COMMIT_HASH}\n",
    "!pip install ./\n",
    "%cd ./..\n",
    "!rm -Rf dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Blg_otRk23ud"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset, visualize_2d_data, visualize_2d_samples\n",
    "from dgm_utils import BaseModel, LabeledDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Snskjc-2_o4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List, Optional\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print('GPU found :)')\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print('GPU not found :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRFti0_GCRpD"
   },
   "source": [
    "## Task 2: DDPM on 2D data (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8vx6pxgCqBv"
   },
   "source": [
    "In this part you have to implement DDPM and apply it to 2D dataset.\n",
    "\n",
    "Let's take a look at dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3U4b0HQ2F06",
    "outputId": "a3d8dfdd-0e64-4f51-a1eb-c6346c7ca01c"
   },
   "outputs": [],
   "source": [
    "COUNT = 20_000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_dataset('moons', size=COUNT, with_targets=True)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRqWAIRmQDQ8"
   },
   "source": [
    "Below you see the utility function, which broadcasts tensors. Look carefully at this code, we will use it in the majority of methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VwXNzi6fQZm"
   },
   "outputs": [],
   "source": [
    "def extract_into_tensor(\n",
    "    arr: torch.Tensor, \n",
    "    indices: torch.Tensor, \n",
    "    broadcast_shape: torch.Size\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D torch tensor for a batch of indices.\n",
    "    :param arr: 1-D torch tensor.\n",
    "    :param timesteps: a tensor of indices to extract from arr.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    assert len(arr.shape) == 1\n",
    "    res = arr.to(device=indices.device)[indices].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "\n",
    "\n",
    "    return res.expand(broadcast_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JGW21W32F07"
   },
   "source": [
    "### Forward Diffusion\n",
    "\n",
    "Let start with forward diffusion.\n",
    "\n",
    "**Forward process** is defined as a posterior distribution $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)$.\n",
    "\n",
    "It is a Markov chain, which consequently adds Gaussian noise to a given object $\\mathbf{x}_0$.\n",
    "\n",
    "At every step of this process the Gaussian noise is added with different magnitude, which is determined with a schedule of variances $\\{\\beta_1, ... \\beta_T\\}$.\n",
    "If this schedule is chosen properly and T goes to infinity (or is large enough), we will converge to pure noise $\\mathcal{N}(0, I)$.\n",
    "\n",
    "Markov chain is defined by:\n",
    "$$\n",
    " q(\\mathbf{x}_t | \\mathbf{x}_{t - 1}) = \\mathcal{N}(\\mathbf{x}_t | \\sqrt{1 - \\beta_t}\\mathbf{x}_{t - 1}, \\beta_t \\mathbf{I}), \\quad q(\\mathbf{x}_{1:T}|\\mathbf{x}_0) = \\prod_{t = 1}^T q(\\mathbf{x}_t | \\mathbf{x}_{t - 1})\n",
    "$$\n",
    "\n",
    "In order to get $\\mathbf{x}_t$ we have to compute $\\mathbf{x}_1, ..., \\mathbf{x}_{t - 1}$ iteratively.\n",
    "\n",
    "Hopefully, due to the properties of the Gaussian distribution we can do it more efficiently.\n",
    "\n",
    "Let's denote\n",
    "$\\alpha_t = 1- \\beta_t$ и $\\bar{\\alpha}_t= \\prod_{s = 1}^t\\alpha_s$.\n",
    "Then\n",
    "$$\n",
    "q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t|\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1-\\bar{\\alpha}_t) \\mathbf{I}).\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Here we could get very useful expression\n",
    "$$\n",
    "    \\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\boldsymbol{\\epsilon}. \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usjLZY1-2F07"
   },
   "source": [
    "Now we will create base class for diffusion (we will use it as a python base class for forward and backward diffusions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPgOkluQ2F07",
    "outputId": "bf60783a-31cc-4254-ec20-ca747ecd844f"
   },
   "outputs": [],
   "source": [
    "class BaseDiffusion:\n",
    "    def __init__(self, num_timesteps: int):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.betas = self._get_beta_schedule(num_timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_beta_schedule(num_diffusion_timesteps: int, s: float = 0.008):\n",
    "        def f(t, T):\n",
    "            return (np.cos((t / T + s) / (1 + s) * np.pi / 2)) ** 2\n",
    "\n",
    "        alphas = []\n",
    "        f0 = f(0, num_diffusion_timesteps)\n",
    "\n",
    "        for t in range(num_diffusion_timesteps + 1):\n",
    "            alphas.append(f(t, num_diffusion_timesteps) / f0)\n",
    "\n",
    "        betas = []\n",
    "\n",
    "        for t in range(1, num_diffusion_timesteps + 1):\n",
    "            betas.append(min(1 - alphas[t] / alphas[t - 1], 0.999))\n",
    "\n",
    "        return torch.from_numpy(np.array(betas)).double()\n",
    "\n",
    "\n",
    "basediff = BaseDiffusion(num_timesteps=20)\n",
    "\n",
    "plt.plot(basediff.betas.numpy())\n",
    "plt.title('Beta schedule')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrGZqa5A2F07"
   },
   "source": [
    "We are ready to define forward diffusion process. It has 2 methods:\n",
    "- to get mean and variance of the distribution $q(\\mathbf{x}_t | \\mathbf{x}_0)$,\n",
    "- to get samples from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3X5Gm1aC3zc"
   },
   "outputs": [],
   "source": [
    "class ForwardDiffusion(BaseDiffusion):\n",
    "    def get_mean_variance(self, x0: torch.Tensor, t: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate mean and variance of the distribution q(x_t | x_0) (use equation (1))\n",
    "        # use extract_into_tensor() function to get tensors of the same shape as x0\n",
    "\n",
    "        # ====\n",
    "        return mean, variance\n",
    "\n",
    "    def get_samples(\n",
    "        self, \n",
    "        x0: torch.Tensor, \n",
    "        t: torch.Tensor, \n",
    "        noise: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample from the distribution q(x_t | x_0) (use equation (2))\n",
    "\n",
    "        # ====\n",
    "        return samples\n",
    "\n",
    "\n",
    "def test_forward_diffusion():\n",
    "    fdiff = ForwardDiffusion(num_timesteps=100)\n",
    "    SHAPE = [2, 20]\n",
    "    x0 = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "    mean, variance = fdiff.get_mean_variance(x0=x0, t=t)\n",
    "    assert list(mean.shape) == SHAPE\n",
    "    assert list(variance.shape) == SHAPE\n",
    "    assert np.allclose(mean.numpy(), np.ones(SHAPE) * 0.9944681)\n",
    "    assert np.allclose(variance.numpy(), np.ones(SHAPE) * 0.01103322)\n",
    "\n",
    "    xt = fdiff.get_samples(x0=x0, t=t)\n",
    "    assert list(xt.shape) == SHAPE\n",
    "\n",
    "    noise = torch.ones(SHAPE)\n",
    "    xt = fdiff.get_samples(x0=x0, t=t, noise=noise)\n",
    "    assert np.allclose(xt.numpy(), np.ones(SHAPE) * 1.0995072)\n",
    "\n",
    "\n",
    "test_forward_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDF9k0r32F07"
   },
   "source": [
    "Let visualize the forward diffusion process. Here you have to see how the distribution of the real samples transforms to the Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3hgwOcGfQZo",
    "outputId": "dfb27928-bf61-45e3-9453-fe1be2dacc18"
   },
   "outputs": [],
   "source": [
    "num_timesteps = 100\n",
    "\n",
    "fdiff = ForwardDiffusion(num_timesteps=num_timesteps)\n",
    "\n",
    "timestamps=[0, 2, 4, 10, 50]\n",
    "\n",
    "plot_n_steps = len(timestamps)\n",
    "for i, t in enumerate(timestamps):\n",
    "    x = fdiff.get_samples(x0=torch.from_numpy(train_data), t=torch.ones((train_data.shape[0], 1)).long() * t)\n",
    "    visualize_2d_samples(x, title=f\"Step of diffusion: {t}\", labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVEnKClW2F08"
   },
   "source": [
    "### Reverse Diffusion\n",
    "\n",
    "**Reverse process** consequently denoises pure Gaussian noise $\\mathcal{N}(0, \\mathbf{I})$ until we do not get the object from the original distribution $\\pi(\\mathbf{x})$.\n",
    "\n",
    "It is a probability model with latent variables\n",
    "$p(\\mathbf{x}_0 | \\boldsymbol{\\theta}) := \\int p(\\mathbf{x}_{0:T} | \\boldsymbol{\\theta}) d\\mathbf{x}_{1:T}$,\n",
    "where\n",
    "- latents $\\mathbf{z} = \\{\\mathbf{x}_1, ..., \\mathbf{x}_T\\}$ correspond to noised objects\n",
    "- $\\mathbf{x}_0$ is an object from the original distribution $\\pi(\\mathbf{x})$.\n",
    "\n",
    "Joint distribution $p(\\mathbf{x}_{0:T} | \\boldsymbol{\\theta})$ is called reverse diffusion process, which is essentially a Markov chain of Gaussian distributions $p(\\mathbf{x}_t|\\mathbf{x}_t, \\boldsymbol{\\theta})$:\n",
    "$$\n",
    "p(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod_{t = 1}^T p(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\boldsymbol{\\theta}), \\quad p(\\mathbf{x}_{T} | \\boldsymbol{\\theta})=\\mathcal{N}(0, \\mathbf{I})\n",
    "$$\n",
    "$$\n",
    "  p(\\mathbf{x}_{t - 1}|\\mathbf{x}_t | \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t), \\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)). \\tag{3}\n",
    "$$\n",
    "\n",
    "In Lecture 9 we have derived ELBO for this model:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(q, \\boldsymbol{\\theta}) =  \\mathbf{E}_{q} \\Bigl[\\log p(\\mathbf{x}_0 | \\mathbf{x}_1, \\boldsymbol{\\theta}) - KL\\bigl(q(\\mathbf{x}_T | \\mathbf{x}_0) || p(\\mathbf{x}_T)\\bigr)\n",
    "    - \\sum_{t=2}^T \\underbrace{KL \\bigl(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) || p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta} )\\bigr)}_{\\mathcal{L}_t} \\Bigr].\n",
    "$$\n",
    "\n",
    "Here we use the following distribution $q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}( \\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}) $, where\n",
    "$$\n",
    "\\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\n",
    "\\tag{4}\n",
    "$$\n",
    "$$\n",
    "\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "(These scary formulas are not difficult to derive, follow the link to find details [Denoising Diffusion Probabilistic Models (Ho et al. 2020)](https://arxiv.org/abs/2006.11239)).\n",
    "\n",
    "Now our goal is to define parameters $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t), \\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)$ of reverse diffusion.\n",
    "\n",
    "#### Variance\n",
    "Our first assumption is to set the variance $\\boldsymbol{\\sigma}^2_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) = \\tilde{\\beta}_t$. This is very native assumption\n",
    "\n",
    "#### Mean\n",
    "Here we will use the expression (2) to get $\\mathbf{x}_0$ from $\\mathbf{x}_t$:\n",
    "$$\n",
    "    \\mathbf{x}_0 = \\frac{\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_{t}} \\cdot \\boldsymbol{\\epsilon}}{\\sqrt{\\bar{\\alpha}_{t}}}.\n",
    "    \\tag{6}\n",
    "$$\n",
    "\n",
    "If we put this expression to the formula (4) we will get:\n",
    "$$\n",
    "    \\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\boldsymbol{\\epsilon} \\right).\n",
    "$$\n",
    "\n",
    "So the idea here to parametrize the model mean in the same functional form:\n",
    "$$\n",
    "    \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) \\right).\n",
    "$$\n",
    "\n",
    "**Note:** our model will predict the noise which was applied to $\\mathbf{x}_0$ to get $\\mathbf{x}_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imt4i-zM2F08",
    "outputId": "401299f2-230f-4d74-f085-b6acb266f34c"
   },
   "outputs": [],
   "source": [
    "class ReverseDiffusion(BaseDiffusion):\n",
    "    def __init__(self, *args, clip_range: Optional[Tuple[float, float]] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.clip_range = clip_range\n",
    "        self.alphas_cumprod_prev = torch.cat(\n",
    "            [torch.tensor([1.0], device=self.betas.device), self.alphas_cumprod[:-1]], dim=0\n",
    "        )\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate variance of the distribution q(x_{t-1} | x_t, x_0) (use equation (5))\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate coefficients of mean of the distribution q(x_{t-1} | x_t, x_0) (use equation (4))\n",
    "        # mean = x_coef * x_t + x0_coef * x_0\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def get_x0(self, xt: torch.Tensor, eps: torch.Tensor, t: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # get x_0 (use equation (6))\n",
    "\n",
    "        # ====\n",
    "        return x0.clamp(*self.clip_range) if self.clip_range is not None else x0\n",
    "\n",
    "    def get_mean_variance(self, xt: torch.Tensor, eps: torch.Tensor, t: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # get mean and variance of the distribution q(x_{t-1} | x_t, x_0) (use equations (4) and (5))        \n",
    "        # use get_x0 method to get x_0\n",
    "\n",
    "        # ====\n",
    "        return mean, variance\n",
    "\n",
    "    def get_samples(self, xt: torch.Tensor, eps: torch.Tensor, t: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get mean and variance of the distribution q(x_{t-1} | x_t, x_0)\n",
    "        # 2) sample noise from the standard normal \n",
    "        # 3) get samples using reparametrization trick\n",
    "\n",
    "        # ====\n",
    "        return sample.float()\n",
    "\n",
    "\n",
    "def test_reverse_diffusion():\n",
    "    rdiff = ReverseDiffusion(num_timesteps=100)\n",
    "    SHAPE = [2, 20]\n",
    "    xt = torch.ones(SHAPE)\n",
    "    eps = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "\n",
    "    x0 = rdiff.get_x0(xt=xt, eps=eps, t=t)\n",
    "    assert list(x0.shape) == SHAPE\n",
    "    assert np.allclose(x0.numpy(), np.ones(SHAPE) * 0.8999391)\n",
    "\n",
    "    mean, variance = rdiff.get_mean_variance(xt=xt, eps=eps, t=t)\n",
    "    assert list(mean.shape) == SHAPE\n",
    "    assert list(variance.shape) == SHAPE\n",
    "    assert np.allclose(mean.numpy(), np.ones(SHAPE) * 0.9723116)\n",
    "    assert np.allclose(variance.numpy(), np.ones(SHAPE) * 0.00222036)\n",
    "\n",
    "    x = rdiff.get_samples(xt, eps, t)\n",
    "    assert list(x.shape) == SHAPE\n",
    "\n",
    "\n",
    "test_reverse_diffusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJuBzzar2F08"
   },
   "source": [
    "### Model\n",
    "\n",
    "In this task we will use simple MLP model to parametrize distribution $p(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\boldsymbol{\\theta})$. It will be conditioned on the timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbtFK3NF2F08"
   },
   "outputs": [],
   "source": [
    "class ConditionalMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_embeds: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.x_proj = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.t_proj = nn.Embedding(num_embeds, self.hidden_dim)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(self.hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.x_proj(x)\n",
    "        t = self.t_proj(t.int())\n",
    "        x = x + t\n",
    "        x = F.selu(x)\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "def test_conditional_mlp():\n",
    "    SHAPE = [2, 20]\n",
    "    T = 100\n",
    "    x = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "    model = ConditionalMLP(input_dim=20, num_embeds=100)\n",
    "    output = model(x, t)\n",
    "    assert list(output.shape) == SHAPE\n",
    "\n",
    "\n",
    "test_conditional_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCXBSZe32F09"
   },
   "source": [
    "### DDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEZ__C82KlzL"
   },
   "source": [
    "Let return to the ELBO. The main part of it is:\n",
    "$$\n",
    "    \\mathcal{L}_t = KL \\bigl(q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0) || p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta} )\\bigr)\n",
    "$$\n",
    "\n",
    "In Lecture 9 we have got that\n",
    "$$\n",
    "    \\mathcal{L}_t = \\mathbf{E}_{\\boldsymbol{\\epsilon}} \\left[ \\frac{\\beta_t^2}{2 \\tilde{\\beta_t} \\alpha_t (1 - \\bar{\\alpha}_t)} \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t) \\|^2 \\right].\n",
    "$$\n",
    "\n",
    "In practice this loss is simplified. Particilarly, we will omit coefficient of the norm and we will sample index $t$ at each training step.\n",
    "\n",
    "Finally, we will train our model with the following objective:\n",
    "$$\n",
    "\\text{loss} = \\mathbf{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}, t}\\bigg[ \\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t)\\|^2\\bigg],\n",
    "$$\n",
    "where $\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqPzHQ_32F09"
   },
   "source": [
    "The following class implements two methods:\n",
    "- `loss` - to compute the loss at the training step;\n",
    "- `sample` - to sample from the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTy1kSEy2F09"
   },
   "outputs": [],
   "source": [
    "class DDPM(BaseModel):\n",
    "    def __init__(self, num_timesteps: int, model: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        self.forward_diffusion = ForwardDiffusion(num_timesteps=num_timesteps)\n",
    "        self.reverse_diffusion = ReverseDiffusion(num_timesteps=num_timesteps)\n",
    "        self.model = model\n",
    "        self.shape = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples: int):\n",
    "        assert self.shape is not None\n",
    "        x = torch.randn((num_samples, *self.shape), device=self.device, dtype=torch.float32)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        for i in indices:\n",
    "            t = torch.tensor([i] * num_samples, device=x.device)\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) get epsilon from the model\n",
    "            # 2) sample from the reverse diffusion\n",
    "\n",
    "            # ====\n",
    "        return x\n",
    "\n",
    "    def loss(self, x0: torch.Tensor):\n",
    "        if self.shape is None:\n",
    "            self.shape = list(x0.shape)[1:]\n",
    "        t = torch.randint(0, self.num_timesteps, size=(x0.size(0),), device=x0.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get x_t\n",
    "        # 2) get epsilon from the model\n",
    "        # 3) compute mse loss between epsilon and noise\n",
    "\n",
    "        # ====\n",
    "        return {\"total_loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw6vueDnTt7C"
   },
   "source": [
    "### Training\n",
    "\n",
    "Now we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "T = \n",
    "# ====\n",
    "\n",
    "model = ConditionalMLP(input_dim=2, num_embeds=T)\n",
    "ddpm = DDPM(num_timesteps=T, model=model)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# try your own optimizer/scheduler\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=LR, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "train_model(\n",
    "    ddpm,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    n_samples=1024,\n",
    "    visualize_samples=True,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ijiAy6pUvkn"
   },
   "source": [
    "Now let's sample from our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ddpm.sample(num_samples=5000).cpu()\n",
    "\n",
    "visualize_2d_samples(samples, title=\"Samples\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RSpn0XJU_G9"
   },
   "source": [
    "Now let's see how denoising looks like (similarly to forward noising process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jXmq3s42F0-",
    "outputId": "56f3ee9e-b069-45ac-9690-4cda3fcf6716"
   },
   "outputs": [],
   "source": [
    "timestamps=[0, 2, 4, 10, 50]\n",
    "\n",
    "x = torch.randn(train_data.shape[0], 2, requires_grad=False).to(ddpm.device)\n",
    "for i in range(ddpm.num_timesteps - 1, -1, -1):\n",
    "    t = torch.tensor(i, dtype=torch.long, requires_grad=False).expand(x.shape[0]).to(ddpm.device)\n",
    "    with torch.no_grad():\n",
    "        eps = ddpm.model(x, t)\n",
    "        x = ddpm.reverse_diffusion.get_samples(xt=x, eps=eps, t=t)\n",
    "    if i in reversed(timestamps):\n",
    "        x_ = x.cpu()\n",
    "        visualize_2d_samples(x_, title=f\"Samples from timestamp: {i}\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXJnkzgiRGRo"
   },
   "source": [
    "## Task3: DDPM on MNIST with guidance (5pt)\n",
    "\n",
    "Let apply our diffusion model to the MNIST dataset with classifier-free guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oktOew8b2F1F",
    "outputId": "a6fc21c2-6862-4c8b-d163-11e41db863be"
   },
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_dataset(\"mnist\", flatten=False, binarize=False, with_targets=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFhOvS0PPZoe"
   },
   "source": [
    "Let's take a look at the forward process for the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydfzM0RU2F1G",
    "outputId": "8317e111-ec29-4180-e7a5-c58207647a6e"
   },
   "outputs": [],
   "source": [
    "num_timesteps = 1000\n",
    "\n",
    "fdiff = ForwardDiffusion(num_timesteps=num_timesteps)\n",
    "\n",
    "timestamps=[0, 50, 100, 200, 300, 500, 600, 800, 999]\n",
    "\n",
    "plot_n_steps = len(timestamps)\n",
    "samples = []\n",
    "x0 = train_data[10:11]\n",
    "x0 = 2 * x0 - 1\n",
    "for i, t in enumerate(timestamps):\n",
    "    x = fdiff.get_samples(x0=torch.from_numpy(x0), t=torch.ones((x0.shape[0], 1)).long() * t)\n",
    "    samples.append(x.cpu().numpy())\n",
    "\n",
    "samples = np.concatenate(samples)\n",
    "samples = (0.5 * samples + 0.5).clip(0, 1)\n",
    "show_samples(samples, title=\"Noisy samples\", nrow=len(timestamps), figsize=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tMXiU8P_6v"
   },
   "source": [
    "As in Seminar 8 we will use UNet - a standart choice for diffusion models. The model will be mostly implemented, but you could change it if you want. \n",
    "\n",
    "First, we'll define embedding layers for our UNet architecture. In diffusion models with guidance, we usually embed both timesteps and conditional information:\n",
    "\n",
    "1. `TimeEmbedding` - creates sinusoidal position embeddings for timesteps, allowing the model to understand which diffusion step it's processing.\n",
    "2. `ConditionalEmbedding` - creates embeddings for class labels, enabling classifier-free guidance where we can condition the generation on specific classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        assert dim_in % 2 == 0\n",
    "        emb = torch.arange(0, dim_in, step=2) / dim_in * math.log(10000)  \n",
    "        emb = torch.exp(emb) \n",
    "        pos = torch.arange(T).float() \n",
    "        emb = pos[:, None] * emb[None, :] \n",
    "        assert list(emb.shape) == [T, dim_in // 2]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1) \n",
    "        assert list(emb.shape) == [T, dim_in // 2, 2]\n",
    "        emb = emb.view(T, dim_in) \n",
    "        \n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb), \n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            Swish(),\n",
    "            nn.Linear(dim_out, dim_out)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(module.weight)\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, t):\n",
    "        emb = self.time_embedding(t)\n",
    "        return emb\n",
    "    \n",
    "class ConditionalEmbedding(nn.Module):\n",
    "    def __init__(self, num_labels, dim_in, dim_out):\n",
    "        assert dim_in % 2 == 0\n",
    "        super().__init__()\n",
    "        self.condition_embedding = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=num_labels+1, embedding_dim=dim_in, padding_idx=0),\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            Swish(),\n",
    "            nn.Linear(dim_out, dim_out),\n",
    "        )\n",
    "        \n",
    "    def forward(self, cond):\n",
    "        emb = self.condition_embedding(cond)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNet's core component is the `ResBlock`, which enhances standard ResNet blocks with time step and conditional inputs embeddings and optional attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_dim)\n",
    "        self.proj_q = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_dim, in_dim, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "        \n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C) \n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        \n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C) \n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "        \n",
    "        return x + h\n",
    "    \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_dim, dropout, attn=False):\n",
    "        super().__init__()\n",
    "             \n",
    "        self.temb_proj = nn.Sequential(Swish(), nn.Linear(t_dim, out_channels))\n",
    "        self.cemb_proj = nn.Sequential(Swish(), nn.Linear(t_dim, out_channels))\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # create two convolutional blocks of nn.Sequential\n",
    "        # first block should take raw input\n",
    "        # second block should take the output of the first block with added time and class embeddings\n",
    "        # it is preferrable to use nn.GroupNorm, Swish and nn.Dropout\n",
    "\n",
    "        # ====\n",
    "\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.attn = AttnBlock(out_channels) if attn else nn.Identity()\n",
    "\n",
    "    \n",
    "    def forward(self, x, temb, labels):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) pass x through the first block\n",
    "        # 2) add time and class embeddings (unsqueeze them to the right shape)\n",
    "        # 3) pass the result through the second block\n",
    "        # 4) add the shortcut\n",
    "        # 5) pass the result through the attention block\n",
    "\n",
    "        # ====\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DownsampleBlock` and `UpsampleBlock` implement the UNet's characteristic encoder-decoder structure, managing resolution changes as features flow through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code \n",
    "        # define convolutional layer that:\n",
    "        # 1) does not change the number of channels\n",
    "        # 2) reduces the size of the image twice\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, *args) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        return x\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code \n",
    "        # define convolutional layer that:\n",
    "        # 1) does not change the number of channels\n",
    "        # 2) does not reduce the size of the image\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, *args) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) upsample the input tensor using bilinear interpolation\n",
    "        # 2) pass the result through the convolutional layer\n",
    "        \n",
    "        # ====\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the complete `UNet` - the neural network backbone of our diffusion model that handles both encoding and decoding with skip connections. Look at the code carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNyTMSl42F1G"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_timesteps: int, \n",
    "        num_labels: int, \n",
    "        hidden_channels: int, \n",
    "        channel_multipliers: List[int], \n",
    "        num_blocks: int = 1,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        temb_dim = hidden_channels * 4\n",
    "        self.time_embedding = TimeEmbedding(num_timesteps, hidden_channels, temb_dim)\n",
    "        self.cond_embedding = ConditionalEmbedding(num_labels, hidden_channels, temb_dim)\n",
    "        self.head = nn.Conv2d(1, hidden_channels, kernel_size=3, stride=1, padding=1)\n",
    "        channel_list, curr_channels = [hidden_channels], hidden_channels\n",
    "\n",
    "        # initialization of downsample blocks\n",
    "        self.downsample_blocks = nn.ModuleList()\n",
    "        for idx, scale in enumerate(channel_multipliers):\n",
    "            out_channels = hidden_channels * scale\n",
    "            is_last = (idx == len(channel_multipliers) - 1)\n",
    "\n",
    "            # make multiple ResBlocks at each scale\n",
    "            for _ in range(num_blocks):\n",
    "                # at each channel_multipliers scale we add ResBlock\n",
    "                # last block has attention\n",
    "                self.downsample_blocks.append(\n",
    "                    ResBlock(curr_channels, out_channels, temb_dim, dropout, attn=True)\n",
    "                )\n",
    "                curr_channels = out_channels\n",
    "                channel_list.append(curr_channels)\n",
    "\n",
    "            # add downsample block if not last block\n",
    "            if not is_last:\n",
    "                self.downsample_blocks.append(DownsampleBlock(curr_channels))\n",
    "                channel_list.append(curr_channels)\n",
    "        \n",
    "        # initialization of bottleneck block\n",
    "        self.bottleneck = nn.ModuleList([\n",
    "            ResBlock(curr_channels, curr_channels, temb_dim, dropout, attn=True),\n",
    "            ResBlock(curr_channels, curr_channels, temb_dim, dropout, attn=False)\n",
    "        ])\n",
    "        \n",
    "        # initialization of upsample blocks\n",
    "        self.upsample_blocks = nn.ModuleList()\n",
    "        for idx, scale in reversed(list(enumerate(channel_multipliers))):\n",
    "            out_channels = hidden_channels * scale\n",
    "            is_first, is_last = (idx == 0), (idx == len(channel_multipliers) - 1)\n",
    "\n",
    "            # make multiple ResBlocks at each scale\n",
    "            for _ in range(num_blocks + 1):\n",
    "                # at each reverse channel_multipliers scale we add ResBlock\n",
    "                # first block has attention\n",
    "                self.upsample_blocks.append(\n",
    "                    ResBlock(channel_list.pop() + curr_channels, out_channels, temb_dim, dropout, attn=False)\n",
    "                )\n",
    "            \n",
    "                curr_channels = out_channels\n",
    "            \n",
    "            # add upsample block if not last block\n",
    "            if not is_first:\n",
    "                self.upsample_blocks.append(UpsampleBlock(curr_channels))\n",
    "                \n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, curr_channels),\n",
    "            Swish(),\n",
    "            nn.Conv2d(curr_channels, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, y=None):\n",
    "        temb = self.time_embedding(t)\n",
    "        cemb = torch.zeros_like(temb) if y is None else self.cond_embedding(y)\n",
    "\n",
    "        h = self.head(x)\n",
    "        skip_connections = [h] # save intermediate results for skip connections\n",
    "\n",
    "        # downsample\n",
    "        for layer in self.downsample_blocks:\n",
    "            h = layer(h, temb, cemb)\n",
    "            skip_connections.append(h) # save intermediate results for skip connections\n",
    "\n",
    "        # bottleneck\n",
    "        for layer in self.bottleneck:\n",
    "            h = layer(h, temb, cemb)\n",
    "        \n",
    "        # upsample\n",
    "        for layer in self.upsample_blocks:\n",
    "            if isinstance(layer, ResBlock): # apply skip connection\n",
    "                skip_connection = skip_connections.pop()\n",
    "                h = torch.cat([h, skip_connection], dim=1)\n",
    "            h = layer(h, temb, cemb)\n",
    "\n",
    "        h = self.tail(h)\n",
    "        return h\n",
    "\n",
    "def test_unet():\n",
    "    model = UNet(\n",
    "        num_timesteps=10, \n",
    "        num_labels=10, \n",
    "        hidden_channels=128, \n",
    "        channel_multipliers=[1, 2, 4], \n",
    "    )\n",
    "    x = torch.rand((2, 1, 32, 32))\n",
    "    y = torch.zeros(size=(2,), dtype=torch.long)\n",
    "    t = torch.zeros(size=(2,), dtype=torch.long)\n",
    "    out1 = model(x, t, y)\n",
    "    t = torch.ones(size=(2,), dtype=torch.long)\n",
    "    out2 = model(x, t, y)\n",
    "    assert not np.allclose(out1.detach().numpy(), out2.detach().numpy())\n",
    "\n",
    "\n",
    "test_unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again implement the class of DDPM but now that can handle classifier-free guidance. \n",
    "\n",
    "From Lecture 10 we know that to train DDPM with classifier-free guidance we train the single model $\\mathbf{\\epsilon}_{\\theta, t}(\\mathbf{x_t}, \\mathbf{y})$ alternating with real conditioning $\\mathbf{y}$ and empty conditioning $\\mathbf{y} = \\emptyset$. Then we apply twice the model to make one denoising step:\n",
    "\n",
    "$$\n",
    "    \\mathbf{x_{t-1}} = \\frac{1}{\\sqrt{\\alpha_t}} \\cdot \\mathbf{x_t} - \\frac{1 - \\alpha_t}{\\sqrt{\\alpha_t (1 - \\bar{\\alpha}_t)}} \\cdot  \\hat{\\mathbf{\\epsilon}}_{\\theta, t}(\\mathbf{x_t}, \\mathbf{y}) + \\sigma_t \\cdot \\mathbf{\\epsilon},\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "    \\hat{\\mathbf{\\epsilon}}_{\\theta, t}(\\mathbf{x_t}, \\mathbf{y}) = \\gamma \\cdot \\mathbf{\\epsilon}_{\\theta, t}(\\mathbf{x_t}, \\mathbf{y}) + (1 - \\gamma) \\cdot \\mathbf{\\epsilon}_{\\theta, t}(\\mathbf{x_t}, \\emptyset).\n",
    "$$\n",
    "\n",
    "**Note 1:** To use UNet in unconditional way use the argument `y = None` (see `UNet.forward`).\n",
    "\n",
    "**Note 2:** We scale the data and clamp final samples $x_0$ to $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDDPM(BaseModel):\n",
    "    def __init__(self, num_timesteps: int, model: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.forward_diffusion = ForwardDiffusion(num_timesteps=num_timesteps)\n",
    "        self.reverse_diffusion = ReverseDiffusion(num_timesteps=num_timesteps, clip_range=(-1.0, 1.0))\n",
    "        self.model = model\n",
    "        self.shape = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples: int, y=None, guidance_scale: float = 3.0):\n",
    "        assert self.shape is not None\n",
    "        x = torch.randn((num_samples, *self.shape), device=self.device, dtype=torch.float32)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        for i in indices:\n",
    "            t = torch.tensor([i] * num_samples, device=x.device)\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) get epsilon from the model without conditioning\n",
    "            # 2) if y is not None then \n",
    "            #    get epsilon from the model with conditioning\n",
    "            #    modify epsilon with classifier-free guidance formula\n",
    "            # 3) sample from the reverse diffusion\n",
    "\n",
    "            # ====\n",
    "        return torch.clamp(0.5 * x + 0.5, 0.0, 1.0)\n",
    "\n",
    "    def loss(self, x0, y=None):\n",
    "        x0 = 2.0 * x0 - 1.0\n",
    "        if self.shape is None:\n",
    "            self.shape = list(x0.shape)[1:]\n",
    "\n",
    "        t = torch.randint(0, self.num_timesteps, size=(x0.shape[0],), device=x0.device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get x_t\n",
    "        # 2) get epsilon from the model\n",
    "        # 3) compute mse loss between epsilon and noise\n",
    "        # NOTE: here you just pass y to the model\n",
    "\n",
    "        # ====\n",
    "        return {\"total_loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V9Xv5wV2F1G"
   },
   "source": [
    "That is all. We are ready to train our **unconditional** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNvEMcANjpMk",
    "outputId": "f1234402-9298-4b2a-a445-13894a817ccb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "HIDDEN_CHANNELS = \n",
    "CHANNEL_MULTIPLIERS = \n",
    "NUM_BLOCKS = \n",
    "T = \n",
    "# ====\n",
    "\n",
    "cond_model = UNet(\n",
    "    num_timesteps=T, \n",
    "    num_labels=10, \n",
    "    hidden_channels=HIDDEN_CHANNELS, \n",
    "    channel_multipliers=CHANNEL_MULTIPLIERS, \n",
    "    num_blocks=NUM_BLOCKS\n",
    ")\n",
    "cond_ddpm = ConditionalDDPM(num_timesteps=T, model=cond_model)\n",
    "\n",
    "# try your own optimizer/scheduler\n",
    "optimizer = torch.optim.AdamW(cond_ddpm.parameters(), lr=LR, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "# train unconditional mode\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_model(\n",
    "    cond_ddpm,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    conditional=False,\n",
    "    device=DEVICE,\n",
    "    n_samples=16,\n",
    "    visualize_samples=True, # prabably you would like to turn this off\n",
    "    logscale_y=True,\n",
    ")\n",
    "# to save your time you can save the model\n",
    "# torch.save(cond_ddpm.state_dict(), \"unconditional_ddpm.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw samples from the trained unconditional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_ddpm.eval()\n",
    "cond_ddpm = cond_ddpm.to(DEVICE)\n",
    "\n",
    "num_samples = 100\n",
    "y = None\n",
    "samples = cond_ddpm.sample(num_samples=num_samples, y=y).cpu().numpy()\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fine-tune our unconditional model on samples with lables to make it **conditional**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "HIDDEN_CHANNELS = \n",
    "CHANNEL_MULTIPLIERS = \n",
    "NUM_BLOCKS = \n",
    "T = \n",
    "# ====\n",
    "\n",
    "# you can load the model with this code\n",
    "# cond_model = UNet(\n",
    "#     num_timesteps=T, \n",
    "#     num_labels=10, \n",
    "#     hidden_channels=HIDDEN_CHANNELS, \n",
    "#     channel_multipliers=CHANNEL_MULTIPLIERS, \n",
    "#     num_blocks=NUM_BLOCKS\n",
    "# )\n",
    "# cond_ddpm = ConditionalDDPM(num_timesteps=T, model=cond_model)\n",
    "# cond_ddpm.load_state_dict(torch.load(\"unconditional_ddpm.pth\"))\n",
    "\n",
    "# optimizer = torch.optim.AdamW(cond_ddpm.parameters(), lr=LR, weight_decay=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "# train conditional mode\n",
    "train_loader = data.DataLoader(LabeledDataset(train_data, train_labels), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(LabeledDataset(test_data, test_labels), batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_model(\n",
    "    cond_ddpm,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    conditional=True,\n",
    "    device=DEVICE,\n",
    "    n_samples=16,\n",
    "    visualize_samples=True,  # prabably you would like to turn this off\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4foGURhyRAaG"
   },
   "source": [
    "Let's draw samples with guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10167,
     "status": "ok",
     "timestamp": 1699721305095,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "j89yASABkOve",
    "outputId": "3acb8522-1f90-46f1-e13c-02d1ef4ba68f"
   },
   "outputs": [],
   "source": [
    "cond_ddpm.eval()\n",
    "cond_ddpm = cond_ddpm.to(DEVICE)\n",
    "\n",
    "num_samples = 100\n",
    "y = (torch.arange(num_samples) % 10).to(DEVICE)\n",
    "samples = cond_ddpm.sample(num_samples=num_samples, y=y).cpu().numpy()\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1419c6699da14696a4dfcfa27645cbca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e2b4a42fa06484685e6dbbb718dd906",
      "placeholder": "​",
      "style": "IPY_MODEL_4bcd480b39ed49ef9fe7b619dac807b8",
      "value": " 10/10 [06:45&lt;00:00, 40.63s/it]"
     }
    },
    "24023cb9b33a47d79482e22c0d78c59f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "344606256fe7408981e4b7c28f17f3ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42e391bd46eb45f08aec7176554e039a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4bcd480b39ed49ef9fe7b619dac807b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "520e625662814b2f99edc701c9903cd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6abe89cddbba4ad0a838389391559646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24023cb9b33a47d79482e22c0d78c59f",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42e391bd46eb45f08aec7176554e039a",
      "value": 10
     }
    },
    "8e2b4a42fa06484685e6dbbb718dd906": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b19b8e868107423d8af000da1d7a0883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed0a4216cd004a9c9ab730676c43d3f2",
       "IPY_MODEL_6abe89cddbba4ad0a838389391559646",
       "IPY_MODEL_1419c6699da14696a4dfcfa27645cbca"
      ],
      "layout": "IPY_MODEL_ee741df5e91a40bba1e091ab4ddae28f"
     }
    },
    "ed0a4216cd004a9c9ab730676c43d3f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_344606256fe7408981e4b7c28f17f3ca",
      "placeholder": "​",
      "style": "IPY_MODEL_520e625662814b2f99edc701c9903cd9",
      "value": "100%"
     }
    },
    "ee741df5e91a40bba1e091ab4ddae28f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
